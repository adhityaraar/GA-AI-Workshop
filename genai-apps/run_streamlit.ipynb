{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -qO - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! streamlit run app.py & yes | npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull llama2\n",
    "# !pip install ollama\n",
    "# ollama run llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# # Specify the model and messages\n",
    "# model = 'llama2'\n",
    "# messages = [\n",
    "#     {\n",
    "#         'role': 'user',\n",
    "#         'content': 'Why is the sky blue?'\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# response_stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "\n",
    "# for chunk in response_stream:\n",
    "#     print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text provided, the policy of Simmons First National Corporation appears to be centered around ethical conduct and transparency. The company has adopted a Code of Ethics that outlines the high ethical standards that all representatives of the corporation are expected to adhere to in their dealings with the public and customers. The code covers various aspects such as dealing with the public, confidentiality, gifts and entertainment, conflicts of interest, and more.\n",
      "\n",
      "The policy emphasizes the importance of representing the Corporation in an ethical manner and ensuring that the reputation and prestige of the company are upheld at all times. It also highlights the role of the Board of Directors in approving and adopting the Code of Ethics, indicating their commitment to maintaining the highest ethical standards within the organization.\n",
      "\n",
      "Overall, the policy appears to prioritize honesty, integrity, and transparency in all business dealings, which can contribute to building trust and credibility with stakeholders and enhance the corporation's reputation in the long run."
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import ollama\n",
    "import logging\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    # Suppress all logging temporarily for this function\n",
    "    logging.getLogger('fitz').setLevel(logging.ERROR)\n",
    "\n",
    "    try:\n",
    "        # Load PDF silently\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        return text  # Return the extracted text instead of creating a document index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_clean_response(model, query):\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response_stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "    \n",
    "    # Stream the response\n",
    "    for chunk in response_stream:\n",
    "        if 'message' in chunk and 'content' in chunk['message']:\n",
    "            yield chunk['message']['content']  # Yield each chunk of the response\n",
    "\n",
    "pdf_path = \"/Users/adhityaraar/Documents/IBM/project/RAG-OS/data/Simmons_Bank.pdf\"\n",
    "extracted_text = process_pdf(pdf_path)\n",
    "\n",
    "if extracted_text:\n",
    "    query = f\"Can you tell me about the policy of the company based on this text: {extracted_text[:1000]}...\"  # Limit for context\n",
    "    model = 'llama2'  # Specify your model\n",
    "\n",
    "    # Print the response as it streams\n",
    "    for response_chunk in get_clean_response(model, query):\n",
    "        print(response_chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, PodSpec\n",
    "from llama_index.llms.huggingface import (\n",
    "    HuggingFaceInferenceAPI,\n",
    "    HuggingFaceLLM,\n",
    ")\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import  Settings\n",
    "# from llama_index.core.storage import StorageContext\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbeddingpyt\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# HF_TOKEN = os.environ.get(\"HUGGING_FACE_TOKEN\", '<HUGGINGFACE_TOKEN>')\n",
    "HF_TOKEN = \"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\"\n",
    "remotely_run = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# locally_run = HuggingFaceLLM(model_name=\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = remotely_run\n",
    "\n",
    "# api_key = os.environ.get(\"PINECONE_API_KEY\",\"<PINECONE_API_KEY>\")\n",
    "api_key = \"65c3f982-2b7d-4288-b6d0-3d26bcd02545\"\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index_name=\"quickstart\"\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=384,\n",
    "#     metric='euclidean',\n",
    "#     spec=PodSpec(environment='gcp-starter', pod_type='s1.x1'),\n",
    "#  )\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
